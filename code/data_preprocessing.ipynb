{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fcb88-e91f-466a-bd78-bd939066170d",
   "metadata": {},
   "source": [
    "# Transaction Data Collection from the Ethereum Blockchain\n",
    "\n",
    "## EIP-1559\n",
    "- Date: August 5, 2021\n",
    "- Block number: 12,965,000\n",
    "- [Ethereum JSON-RPC Specification](https://ethereum.github.io/execution-apis/api-documentation/)\n",
    "- [JSON RPC API](https://ethereum.org/en/developers/docs/apis/json-rpc/)\n",
    "- [EIP-1559 Analysis Arxiv](https://github.com/SciEcon/EIP1559)\n",
    "\n",
    "## Layer 2 Solutions Launch Dates\n",
    "Source: [L2BEAT](https://l2beat.com/scaling/tvl)\n",
    "1. Optimism is live on: January 16, 2021\n",
    "2. Arbitrum is live on: August 31, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d46b85a-a478-4e4c-b1f5-0ccc63b33ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600db6e9-0b3a-4705-975d-7b3c6fc6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Convert a date string to a Unix timestamp.\n",
    "    \n",
    "    Args:\n",
    "        date_string (str): The date string in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        int: The Unix timestamp corresponding to the date string.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    return int(dt.timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7723227-ba57-44b9-b69c-fe20224672df",
   "metadata": {},
   "source": [
    "### Merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa80e15-f235-44ae-9090-90feb8222b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory you want to start from\n",
    "rootDir = '../data/'  # current directory; adjust it if needed\n",
    "\n",
    "# Find all CSV files in the directory\n",
    "all_files = glob.glob(os.path.join(rootDir, \"eth_transaction_data_*.csv\"))\n",
    "\n",
    "# Read and concatenate all files into one DataFrame\n",
    "merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "merged_data.to_csv('merged_eth_transaction_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1037f-f852-4c2e-bec3-f331f61913fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_senders(data):\n",
    "    \"\"\"\n",
    "    Collect all unique sender addresses from a dataframe of transaction data.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): A dataframe representing transaction data.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A dataframe of unique sender addresses.\n",
    "    \"\"\"\n",
    "    unique_senders = data['Sender\\'s Address'].unique()\n",
    "    return pd.DataFrame(unique_senders, columns=['Sender\\'s Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f888f8-d3e8-4fa0-b320-33d7d1975c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('merged_eth_transaction_data.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948df8b-ee1e-46e4-bd98-253d6ebc4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6edbf83-7b49-487c-8527-e545c91b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique sender addresses\n",
    "unique_senders = get_unique_senders(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308f9a8-e4da-40ae-a033-bf4dde600c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save the unique senders to a CSV file\n",
    "unique_senders.to_csv('unique_senders.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bf33a-928c-452d-a96a-46f7e88e1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to pandas Timestamp\n",
    "df['transaction_timestamp'] = pd.to_datetime(df['transaction_timestamp'], unit='s')\n",
    "\n",
    "# Sort by user address and timestamp\n",
    "df.sort_values(['senders_address', 'transaction_timestamp'], inplace=True)\n",
    "\n",
    "# Add a column for transaction count within the last 12 hours, initialized with 0\n",
    "df['trans_freq'] = 0\n",
    "\n",
    "# Initialize the dictionary to store transaction counts\n",
    "transaction_counts = defaultdict(list)\n",
    "\n",
    "# Loop over each row in the DataFrame\n",
    "for i in range(len(df)):\n",
    "    # Get the current user and transaction timestamp\n",
    "    current_user = df.iloc[i]['senders_address']\n",
    "    current_time = df.iloc[i]['transaction_timestamp']\n",
    "\n",
    "    # Define the 12-hour window start time\n",
    "    window_start = current_time - pd.Timedelta(hours=12)\n",
    "\n",
    "    # Store transaction timestamps for each user\n",
    "    transaction_counts[current_user].append(current_time)\n",
    "\n",
    "    # Keep only transactions within the 12-hour window\n",
    "    transaction_counts[current_user] = [ts for ts in transaction_counts[current_user] if ts >= window_start]\n",
    "\n",
    "    # Set the transaction frequency for the current row to the number of transactions within the time window\n",
    "    df.at[i, 'trans_freq'] = len(transaction_counts[current_user])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d67dce-a564-4fd1-8a9d-2065b214c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61692649-ebd6-4d63-a302-7d7f7afa00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to datetime and create a temporary column\n",
    "df['temp_timestamp'] = pd.to_datetime(df['transaction_timestamp'])\n",
    "\n",
    "# Set the launch dates for Layer 2 solutions\n",
    "optimism_launch = datetime(2021, 1, 16)\n",
    "arbitrum_launch = datetime(2021, 8, 31)\n",
    "\n",
    "# Add the 'layer2_availability' column\n",
    "df['layer2_availability'] = ((df['temp_timestamp'] >= arbitrum_launch) | (df['temp_timestamp'] >= optimism_launch)).astype(int)\n",
    "\n",
    "# Add the 'post_eip1559' column. \n",
    "# EIP-1559 went live on August 5, 2021\n",
    "eip1559_launch_date = datetime(2021, 8, 5)\n",
    "df['post_eip1559'] = (df['temp_timestamp'] >= eip1559_launch_date).astype(int)\n",
    "\n",
    "# Drop the temporary column\n",
    "df = df.drop('temp_timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb7baa-46fb-4e3e-a0f0-48bd7972a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd44fa-3a18-43f8-94d7-39f749cb6d8b",
   "metadata": {},
   "source": [
    "---\n",
    "1. **User transaction count**: The cumulative number of transactions made by the same user up to the current transaction.\n",
    "2. **Failed transactions**: the cumulative number of failed transactions by the same user up to the current transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57520029-c8ee-49ad-bb68-30d54a81c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that is 1 if the transaction failed and 0 otherwise\n",
    "df['is_failed'] = (df['transaction_status'] == 'False').astype(int)\n",
    "\n",
    "# Group by user address and calculate the cumulative count of transactions and failed transactions\n",
    "df = df.sort_values('transaction_timestamp')\n",
    "df['user_transaction_count'] = df.groupby('senders_address').cumcount() + 1\n",
    "df['failed_transactions'] = df.groupby('senders_address')['is_failed'].cumsum()\n",
    "\n",
    "# You can drop the 'is_failed' column if you no longer need it\n",
    "df = df.drop('is_failed', axis=1)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee14327-23a5-4ffa-a296-22e58a102fdf",
   "metadata": {},
   "source": [
    "In this script, we first created a new column `is_failed` that is 1 if the transaction failed and 0 otherwise. We then sorted the dataframe by the transaction timestamp to ensure transactions are processed in the order they occurred. \n",
    "\n",
    "Next, we grouped the dataframe by the sender's address and used `cumcount` to get the cumulative count of transactions for each user (we added 1 because `cumcount` starts from 0). We also used `cumsum` on the `is_failed` column to get the cumulative count of failed transactions for each user. \n",
    "\n",
    "Finally, we dropped the `is_failed` column as it's no longer needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c9adf-b010-49c8-b17f-412c7e1068e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ec8da-9779-419d-8478-4b3517aa50ff",
   "metadata": {},
   "source": [
    "---\n",
    "Derive User Address Age\n",
    "\n",
    "We are using 'eth.getTransactionCount' function from web3.py, which retrieves the number of transactions sent from an address up until a certain block. Using this function, we can find the block at which the address was first used.\n",
    "\n",
    "\n",
    "In the script below, for each transaction in the dataframe, the script scans blocks from the genesis block to the block of the transaction. If the address has made any transactions up to a block, it means that this block is the first usage block of the address, so it's saved to the `first_usage` dictionary. The `get_transaction_count` function is used to get the number of transactions sent from an address up to a certain block. After finding the first usage block of each address, the script adds a new column 'address_age' to the data, which represents the age of the address at the time of each transaction.\n",
    "\n",
    "The above solution is likely to be slow if you have a large number of unique addresses and transactions. It's because for each address, it scans blocks from the genesis block to the block of each transaction of the address. If you have a large number of unique addresses, the total number of blocks to scan can be enormous.\n",
    "\n",
    "To optimize the calculation of user address age and utilize a high-performance computing cluster, we could use parallel computing techniques. This involves dividing the computation task into smaller jobs that can be run simultaneously across multiple processors or nodes in the cluster.\n",
    "\n",
    "In Python, several libraries allow you to use parallel computing, such as `multiprocessing`, `concurrent.futures`, and `joblib`. Here's an example of how you could use the `concurrent.futures` library to parallelize the block scanning task. This script uses a `ThreadPoolExecutor` to run multiple block scanning tasks simultaneously, which should significantly speed up the process if you're running it on a machine with multiple CPU cores. However, the maximum number of concurrent tasks is limited by the number of CPU cores in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cf3ab-0492-4c76-9cbb-d14bb4a1e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the first usage block of an address\n",
    "def find_first_usage(address, transaction_block):\n",
    "    # Scan blocks from the start block to the transaction block\n",
    "    for block in range(start_block, transaction_block):\n",
    "        # If the address has made any transactions up to the block\n",
    "        if web3.eth.get_transaction_count(address, block):\n",
    "            return block\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c5c3f-0bf0-490f-9865-9dcb3a5e2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by block number\n",
    "df.sort_values(['block_number'], inplace=True)\n",
    "\n",
    "# Prepare a dictionary to store the first usage block of each address\n",
    "first_usage = {}\n",
    "\n",
    "# Define the range of blocks to scan\n",
    "start_block = 0  # This should be set to the genesis block\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # For each unique address in the data\n",
    "    for address in df['senders_address'].unique():\n",
    "        transaction_block = df[df['senders_address'] == address]['block_number'].min()\n",
    "        \n",
    "        # If the first usage block of the address has already been found, skip this address\n",
    "        if address in first_usage:\n",
    "            continue\n",
    "        \n",
    "        # Submit a new task to the executor\n",
    "        future = executor.submit(find_first_usage, address, transaction_block)\n",
    "        \n",
    "        # Store the Future object in the dictionary\n",
    "        first_usage[address] = future\n",
    "\n",
    "# Retrieve the results from the Future objects\n",
    "for address, future in first_usage.items():\n",
    "    first_usage[address] = future.result()\n",
    "\n",
    "# Add a new column 'address_age' to the data\n",
    "df['address_age'] = df['senders_address'].map(first_usage)\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df0f47-2c25-41bd-b611-7cb62a8e7711",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Time of the day variable**\n",
    "\n",
    "Due to the global nature of Ethereum transactions, the \"time of day\" variable could have different implications for users in different time zones, and this could indeed introduce complexity and potentially confounding effects into the analysis.\n",
    "\n",
    "However, there may still be value in including a \"time of day\" variable, even in a global context, for several reasons:\n",
    "\n",
    "1. **Network Effects**: Blockchain networks like Ethereum may experience periods of higher and lower congestion, which could align with certain times of day, despite global usage. For example, if a substantial proportion of Ethereum users are based in a particular region (say, North America or East Asia), then the network might be busier during the waking hours of that region.\n",
    "\n",
    "2. **Market Activity**: Cryptocurrency markets operate 24/7 and market activity (trading volume, price volatility, etc.) can vary significantly across different times of the day, potentially influencing user behavior. For example, users might be more likely to engage in DeFi transactions during periods of high market activity.\n",
    "\n",
    "3. **Behavioral Patterns**: Regardless of the global nature of Ethereum, there might be common daily behavioral patterns. For example, users might be more active during their daytime and less active during their nighttime, and these patterns could aggregate up to observable patterns in the data.\n",
    "\n",
    "However, given the global nature, it may be advisable to construct the \"time of day\" variable in a way that captures potential global effects. For example, you could split the day into fewer, larger chunks (like morning, afternoon, evening, and night), or analyze this variable carefully in your exploratory analysis to understand its distribution and potential impacts.\n",
    "\n",
    "Another approach might be to construct a variable that captures the \"local time of day\" for each transaction, assuming you can infer or have information about the geographic location of each user (which could introduce privacy issues and may not be feasible). But again, these are more complex and may not necessarily offer a better representation.\n",
    "\n",
    "Finally, including \"time of day\" in your initial model does not obligate you to keep it in your final model. If exploratory analysis or preliminary model results suggest it's not meaningful or is causing issues, you can always exclude it in later iterations of your modeling process.\n",
    "\n",
    "In conclusion, the \"time of day\" could potentially be a meaningful variable in your analysis, but its use and interpretation require careful consideration due to the global nature of Ethereum usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073c20-533b-4862-a2f6-3ac84973fb97",
   "metadata": {},
   "source": [
    "Given the global nature of Ethereum, a simple division of a day into distinct periods based on a specific time zone may not be the most representative. However, a potential solution could be to divide the day into a number of periods that are likely to capture significant changes in activity. For instance:\n",
    "\n",
    "1. **Daytime**: 06:00 to 17:59\n",
    "2. **Evening**: 18:00 to 21:59\n",
    "3. **Night**: 22:00 to 05:59\n",
    "\n",
    "This division attempts to capture typical working hours (daytime), after-work hours (evening), and sleeping hours (night). Of course, given the global nature of the network, these periods won't align perfectly with these times for all users, but they might serve as a useful approximation.\n",
    "\n",
    "If it's possible to incorporate additional information, such as the geographic distribution of Ethereum users or the times of day that tend to see the most network activity, this could be used to refine these periods further.\n",
    "\n",
    "You can then create a new variable, \"TimeOfDay\", in your dataset by mapping each transaction timestamp to one of these periods.\n",
    "\n",
    "Here's a Python script to do that assuming your timestamp is in the form 'YYYY-MM-DD HH:MM:SS' and in UTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11eef1-b935-4cef-93b8-89357704b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_time_of_day(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    if 6 <= hour < 18:\n",
    "        return 'Daytime'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['time_of_day'] = df['transaction_timestamp'].apply(assign_time_of_day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jom-blockchain",
   "language": "python",
   "name": "jom-blockchain-research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
