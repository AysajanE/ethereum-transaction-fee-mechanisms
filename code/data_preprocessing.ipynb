{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fcb88-e91f-466a-bd78-bd939066170d",
   "metadata": {},
   "source": [
    "# Transaction Data Collection from the Ethereum Blockchain\n",
    "\n",
    "## EIP-1559\n",
    "- Date: August 5, 2021\n",
    "- Block number: 12,965,000\n",
    "- [Ethereum JSON-RPC Specification](https://ethereum.github.io/execution-apis/api-documentation/)\n",
    "- [JSON RPC API](https://ethereum.org/en/developers/docs/apis/json-rpc/)\n",
    "- [EIP-1559 Analysis Arxiv](https://github.com/SciEcon/EIP1559)\n",
    "\n",
    "## Layer 2 Solutions Launch Dates\n",
    "Source: [L2BEAT](https://l2beat.com/scaling/tvl)\n",
    "1. Optimism is live on: January 16, 2021\n",
    "2. Arbitrum is live on: August 31, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d46b85a-a478-4e4c-b1f5-0ccc63b33ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600db6e9-0b3a-4705-975d-7b3c6fc6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Convert a date string to a Unix timestamp.\n",
    "    \n",
    "    Args:\n",
    "        date_string (str): The date string in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        int: The Unix timestamp corresponding to the date string.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    return int(dt.timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7723227-ba57-44b9-b69c-fe20224672df",
   "metadata": {},
   "source": [
    "### Merge the eth tx data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9052402-ced9-411c-bd07-04c099fdd0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your files are stored\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "# Filename pattern\n",
    "filename_pattern = \"eth_transaction_data_{}.csv\"\n",
    "\n",
    "# Find all filenames in the directory\n",
    "all_files = os.listdir(data_dir)\n",
    "\n",
    "# Extract dates from filenames and convert to datetime\n",
    "dates = [datetime.strptime(re.search(r'\\d{4}-\\d{2}-\\d{2}', file).group(), \"%Y-%m-%d\") for file in all_files if re.search(r'\\d{4}-\\d{2}-\\d{2}', file)]\n",
    "\n",
    "# Find start and end dates\n",
    "start_date = min(dates)\n",
    "end_date = max(dates)\n",
    "\n",
    "print(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539db4e-cc99-41c4-afca-0acc5dd99f57",
   "metadata": {},
   "source": [
    "In this script above:\n",
    "\n",
    "1. The `os.listdir` function is used to retrieve all the filenames in the directory.\n",
    "2. The `re.search` function is used to extract the date strings from the filenames using a regular expression that matches the date format (yyyy-mm-dd).\n",
    "3. The `datetime.strptime` function is used to convert the date strings to datetime objects.\n",
    "4. The `min` and `max` functions are used to find the start and end dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ecde9-cd57-4c86-b4fb-e5c4f30eff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to indicate whether it's the first file\n",
    "first_file = True\n",
    "\n",
    "# Create or open the final CSV file in append mode\n",
    "with open('../data/merged_eth_transaction_data.csv', 'a') as singleFile:\n",
    "    for single_date in pd.date_range(start_date, end_date):\n",
    "        filename = os.path.join(data_dir, filename_pattern.format(single_date.strftime(\"%Y-%m-%d\")))\n",
    "        \n",
    "        if os.path.isfile(filename):  # if the file exists\n",
    "            df = pd.read_csv(filename, dtype={5: float})\n",
    "            # Write data to file\n",
    "            if first_file:  # If it's the first file\n",
    "                df.to_csv(singleFile, header=True)  # Write with header\n",
    "                first_file = False  # After the first file, set this flag to False\n",
    "            else:\n",
    "                df.to_csv(singleFile, header=False, mode='a')  # If not the first file, write without header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2c148-0665-4d11-8360-eaa11a737bb1",
   "metadata": {},
   "source": [
    "In this script for merging transtion data:\n",
    "1. It iterates over the date range, reads the data for each date into a DataFrame, and appends it to the final CSV file.\n",
    "2. a flag `first_file` is used to check if the current file is the first one. If it is, the script writes the DataFrame to the CSV file with headers. For subsequent files, the DataFrame is written without headers. The `mode='a'` argument to `to_csv` is used to append the data to the existing file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150d89b-a6e3-465f-88c6-ea58ee3e18b1",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Given the large size of your data, traditional Python tools like Pandas might not be able to handle it efficiently due to memory constraints. Fortunately, there are tools and libraries built specifically for handling larger-than-memory datasets, such as Dask and Vaex.\n",
    "\n",
    "1. **Dask**: Dask is a flexible library for parallel computing in Python that's built on top of existing Python APIs and data structures, like NumPy arrays and Pandas DataFrames. It can handle larger-than-memory computations by breaking them down into smaller tasks, executing these tasks in parallel and combining the results.\n",
    "\n",
    "\n",
    "2. **Vaex**: Vaex is another library for handling large datasets. Like Dask, it performs lazy evaluations and only reads in data when necessary. Its API is also similar to Pandas, which makes it easy to use if you're familiar with Pandas:\n",
    "\n",
    "In either case, you should ensure that your machine has enough storage to hold the intermediate results of the computations. Also, both Dask and Vaex make use of multiple cores, so having a multi-core machine can speed up the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000c058-698d-4b52-ae66-3798a44c77af",
   "metadata": {},
   "source": [
    "In Dask, the way to read and modify columns is similar to pandas. You can use Dask's `read_csv` function to read the CSV file and then apply the string operations to the column names using the `rename` method. Here's how you can adapt your code to use Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3578e4f4-213f-462f-8808-54a9d7cc5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Transaction Identifier', 'Block Number',\n",
      "       'Transaction Timestamp', 'Transaction Status', 'Gas Price',\n",
      "       'Transaction Fee', 'Sender's Address', 'Transaction Type',\n",
      "       'Transaction EIP-1559 Type'],\n",
      "      dtype='object')\n",
      "Unnamed: 0                     int64\n",
      "Transaction Identifier        object\n",
      "Block Number                   int64\n",
      "Transaction Timestamp          int64\n",
      "Transaction Status            object\n",
      "Gas Price                      int64\n",
      "Transaction Fee              float64\n",
      "Sender's Address              object\n",
      "Transaction Type              object\n",
      "Transaction EIP-1559 Type     object\n",
      "dtype: object\n",
      "Index(['transaction_identifier', 'block_number', 'transaction_timestamp',\n",
      "       'transaction_status', 'gas_price', 'transaction_fee', 'senders_address',\n",
      "       'transaction_type', 'transaction_eip-1559_type'],\n",
      "      dtype='object')\n",
      "transaction_identifier        object\n",
      "block_number                   int64\n",
      "transaction_timestamp          int64\n",
      "transaction_status            object\n",
      "gas_price                      int64\n",
      "transaction_fee              float64\n",
      "senders_address               object\n",
      "transaction_type              object\n",
      "transaction_eip-1559_type     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Specify the data types\n",
    "dtypes = {\n",
    "    'Transaction Identifier': str,\n",
    "    'Transaction Status': str,\n",
    "    'Sender\\'s Address': str,\n",
    "    'Transaction Type': str,\n",
    "    'Transaction EIP-1559 Type': str\n",
    "}\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = dd.read_csv('../data/merged_eth_transaction_data.csv', dtype=dtypes)\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "# If 'unnamed: 0' column exists, drop it\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Format column names to replace spaces with '_' and replace single quotes with \"\".\n",
    "df = df.rename(columns={col: col.lower().replace(' ', '_').replace(\"'\", \"\") for col in df.columns})\n",
    "\n",
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce18a66-a2ab-4d1b-b00d-046c95ec0dcc",
   "metadata": {},
   "source": [
    "Dask's `read_csv` function returns a Dask DataFrame, which is a large parallel DataFrame composed of smaller pandas DataFrames, split along the index. These pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster.\n",
    "\n",
    "One important thing to remember is that Dask uses lazy execution, which means it only executes tasks when absolutely necessary. So if you're not seeing any output or the code seems to be running very quickly, it's likely because Dask hasn't actually performed the computations yet.\n",
    "\n",
    "To actually perform the computations and get the result, you need to call the `compute` method. However, be careful when using `compute` with large datasets, because it loads the result into memory, which could cause your program to run out of memory if the result is too large. In your case, you won't need to use `compute` until you actually need to see the modified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94041f3-610b-41fe-b237-3f14427321c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_identifier</th>\n",
       "      <th>block_number</th>\n",
       "      <th>transaction_timestamp</th>\n",
       "      <th>transaction_status</th>\n",
       "      <th>gas_price</th>\n",
       "      <th>transaction_fee</th>\n",
       "      <th>senders_address</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>transaction_eip-1559_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x804bfa175a2a32b3ed330c7c642ac2210784e1646e39...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>269000000000</td>\n",
       "      <td>2.824500e+16</td>\n",
       "      <td>0xb0da6794da4E6f7244B96256AdB8973D07428a20</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2e9c29c7b00fedbf30d0e3f4c4d4039da468b46bdb27...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>246015000000</td>\n",
       "      <td>1.833599e+16</td>\n",
       "      <td>0x274F3c32C90517975e29Dfc209a23f315c1e5Fc7</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xb78c319c2e68e82be6a0fb34cd49c17e65f1304ea7dc...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>209000000000</td>\n",
       "      <td>4.389000e+15</td>\n",
       "      <td>0xf6292B422EaA44165519480Ebc232eEfB9F0a47f</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x9e26892415c6643b1728a5c6048a2521346092287276...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>209000000000</td>\n",
       "      <td>1.463000e+17</td>\n",
       "      <td>0xB630D90e3EBE206d2C2A36C060D14812E320862e</td>\n",
       "      <td>Interaction with a Contract</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xa6798928631b5e101b5dd27d123122b125e8ce5f9a6b...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>4.347000e+15</td>\n",
       "      <td>0x741c8EE1d80738dBff66cb914cF74763cc7a0f89</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x16286165e0637f5acbe2cbb2c565e3d54aa463431616...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>209000000000</td>\n",
       "      <td>4.389000e+15</td>\n",
       "      <td>0x741c8EE1d80738dBff66cb914cF74763cc7a0f89</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x9cd1cf12a60507743a89f2c3d367333bc5a89bc51adb...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>5.175000e+16</td>\n",
       "      <td>0x1A579A8FeCE2dcA0E74A12Cc350220178A353D25</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x665ff9b7c7b89fe8793eda31aecb7ff01b9723e4b7d3...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>5.175000e+16</td>\n",
       "      <td>0x662F7fEaC9401d403Ddb0972ff85270071009b9F</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x20f2c1f815341f6fa822242dc7b2aad5dbe92d1ef474...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>5.175000e+16</td>\n",
       "      <td>0x55ac3377c5B65d03866895f81B241eaF18221A8D</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0xa9eec9ad774dce95e9395f9787b080437bc081396af2...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>4.347000e+15</td>\n",
       "      <td>0x6fAf75112F9Ee5e7A40328F8e64041c4399a2Dc8</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              transaction_identifier  block_number  \\\n",
       "0  0x804bfa175a2a32b3ed330c7c642ac2210784e1646e39...      11794239   \n",
       "1  0x2e9c29c7b00fedbf30d0e3f4c4d4039da468b46bdb27...      11794239   \n",
       "2  0xb78c319c2e68e82be6a0fb34cd49c17e65f1304ea7dc...      11794239   \n",
       "3  0x9e26892415c6643b1728a5c6048a2521346092287276...      11794239   \n",
       "4  0xa6798928631b5e101b5dd27d123122b125e8ce5f9a6b...      11794239   \n",
       "5  0x16286165e0637f5acbe2cbb2c565e3d54aa463431616...      11794239   \n",
       "6  0x9cd1cf12a60507743a89f2c3d367333bc5a89bc51adb...      11794239   \n",
       "7  0x665ff9b7c7b89fe8793eda31aecb7ff01b9723e4b7d3...      11794239   \n",
       "8  0x20f2c1f815341f6fa822242dc7b2aad5dbe92d1ef474...      11794239   \n",
       "9  0xa9eec9ad774dce95e9395f9787b080437bc081396af2...      11794239   \n",
       "\n",
       "   transaction_timestamp transaction_status     gas_price  transaction_fee  \\\n",
       "0             1612501201            Success  269000000000     2.824500e+16   \n",
       "1             1612501201            Success  246015000000     1.833599e+16   \n",
       "2             1612501201            Success  209000000000     4.389000e+15   \n",
       "3             1612501201            Success  209000000000     1.463000e+17   \n",
       "4             1612501201            Success  207000000000     4.347000e+15   \n",
       "5             1612501201            Success  209000000000     4.389000e+15   \n",
       "6             1612501201            Success  207000000000     5.175000e+16   \n",
       "7             1612501201            Success  207000000000     5.175000e+16   \n",
       "8             1612501201            Success  207000000000     5.175000e+16   \n",
       "9             1612501201            Success  207000000000     4.347000e+15   \n",
       "\n",
       "                              senders_address             transaction_type  \\\n",
       "0  0xb0da6794da4E6f7244B96256AdB8973D07428a20               ERC20 Transfer   \n",
       "1  0x274F3c32C90517975e29Dfc209a23f315c1e5Fc7               ERC20 Transfer   \n",
       "2  0xf6292B422EaA44165519480Ebc232eEfB9F0a47f        Simple Ether Transfer   \n",
       "3  0xB630D90e3EBE206d2C2A36C060D14812E320862e  Interaction with a Contract   \n",
       "4  0x741c8EE1d80738dBff66cb914cF74763cc7a0f89        Simple Ether Transfer   \n",
       "5  0x741c8EE1d80738dBff66cb914cF74763cc7a0f89        Simple Ether Transfer   \n",
       "6  0x1A579A8FeCE2dcA0E74A12Cc350220178A353D25               ERC20 Transfer   \n",
       "7  0x662F7fEaC9401d403Ddb0972ff85270071009b9F               ERC20 Transfer   \n",
       "8  0x55ac3377c5B65d03866895f81B241eaF18221A8D               ERC20 Transfer   \n",
       "9  0x6fAf75112F9Ee5e7A40328F8e64041c4399a2Dc8        Simple Ether Transfer   \n",
       "\n",
       "  transaction_eip-1559_type  \n",
       "0                    Legacy  \n",
       "1                    Legacy  \n",
       "2                    Legacy  \n",
       "3                    Legacy  \n",
       "4                    Legacy  \n",
       "5                    Legacy  \n",
       "6                    Legacy  \n",
       "7                    Legacy  \n",
       "8                    Legacy  \n",
       "9                    Legacy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe7b8d-6c54-4a91-b49f-3c08464050aa",
   "metadata": {},
   "source": [
    "### An alternative approach to merge data from multiple csv files\n",
    "The script below is more suitable for a smaller number of files or if your machine has a large amount of memory available. We will use this script to merge eth net supply data for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00426b70-d9ea-44c0-a266-0d0bb33e2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n"
     ]
    }
   ],
   "source": [
    "# Set the directory you want to start from\n",
    "rootDir = '../data/'  # current directory; adjust it if needed\n",
    "\n",
    "# Find all eth net supply CSV files in the directory\n",
    "all_files = glob.glob(os.path.join(rootDir, \"eth_net_supply_*.csv\"))\n",
    "\n",
    "# Use sort to ensure files are concatenated in the correct order\n",
    "all_files.sort()\n",
    "\n",
    "# Use a generator expression to lazily read and concatenate files\n",
    "merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
    "\n",
    "dtypes = {\n",
    "    'block_number': 'int',\n",
    "    'static_reward': 'float',\n",
    "    'uncle_reward': 'float',\n",
    "    'inclusion_reward': 'float',\n",
    "    'base_fee': 'int',\n",
    "    'gas_used': 'int',\n",
    "    'burned_fee': 'int',\n",
    "    'net_supply_increase': 'float'\n",
    "}\n",
    "\n",
    "# Convert types for each column\n",
    "for column, dtype in dtypes.items():\n",
    "    # Convert the column to numeric values, making non-convertible values NaN\n",
    "    merged_data[column] = pd.to_numeric(merged_data[column], errors='coerce')\n",
    "\n",
    "    # Now, drop the rows where we got NaN values due to failed conversion\n",
    "    merged_data = merged_data[merged_data[column].notna()]\n",
    "\n",
    "    # And finally, convert the column to the target type\n",
    "    merged_data[column] = merged_data[column].astype(dtype)\n",
    "\n",
    "merged_data.to_csv('merged_eth_net_supply.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3bf509-9272-4e93-84e4-8a0516cef196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['block_number', 'static_reward', 'uncle_reward', 'inclusion_reward',\n",
      "       'base_fee', 'gas_used', 'burned_fee', 'net_supply_increase'],\n",
      "      dtype='object')\n",
      "block_number             int64\n",
      "static_reward          float64\n",
      "uncle_reward           float64\n",
      "inclusion_reward       float64\n",
      "base_fee                 int64\n",
      "gas_used                 int64\n",
      "burned_fee               int64\n",
      "net_supply_increase    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)\n",
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18f042f4-c123-452c-ab88-d3ef539b03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_senders(input_file, output_file, sender_column):\n",
    "    \"\"\"\n",
    "    Function to extract unique sender addresses from a large CSV file of transaction data using Dask.\n",
    "\n",
    "    Parameters:\n",
    "    input_file (str): Path to the input CSV file representing transaction data.\n",
    "    output_file (str): Path to the output CSV file where unique sender addresses will be saved.\n",
    "    sender_column (str): The name of the column in the input file that contains the sender's addresses.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in data using Dask's read_csv function\n",
    "    # Dask is a parallel computing library that allows us to work with large datasets\n",
    "    # The read_csv function works similarly to pandas' read_csv, but it performs the operations lazily\n",
    "    df = dd.read_csv(input_file)\n",
    "\n",
    "    # Get unique sender's addresses\n",
    "    # drop_duplicates returns the unique values in the sender_column\n",
    "    unique_senders = df[sender_column].drop_duplicates()\n",
    "\n",
    "    # Compute the result and save to a new CSV file\n",
    "    # compute() performs the actual computation and returns a pandas DataFrame\n",
    "    # to_csv writes the DataFrame to a CSV file\n",
    "    unique_senders.compute().to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca46eca-7ea9-4431-8950-24c15aee8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "extract_unique_senders('../data/merged_eth_transaction_data.csv', 'unique_senders.csv', 'Sender\\'s Address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1037f-f852-4c2e-bec3-f331f61913fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_senders(data):\n",
    "    \"\"\"\n",
    "    Collect all unique sender addresses from a dataframe of transaction data.\n",
    "    \n",
    "    Args:\n",
    "        data (DataFrame): A dataframe representing transaction data.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A dataframe of unique sender addresses.\n",
    "    \"\"\"\n",
    "    unique_senders = data['Sender\\'s Address'].unique()\n",
    "    return pd.DataFrame(unique_senders, columns=['Sender\\'s Address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6edbf83-7b49-487c-8527-e545c91b1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique sender addresses\n",
    "unique_senders = get_unique_senders(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308f9a8-e4da-40ae-a033-bf4dde600c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save the unique senders to a CSV file\n",
    "unique_senders.to_csv('unique_senders.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bf33a-928c-452d-a96a-46f7e88e1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to pandas Timestamp\n",
    "df['transaction_timestamp'] = pd.to_datetime(df['transaction_timestamp'], unit='s')\n",
    "\n",
    "# Sort by user address and timestamp\n",
    "df.sort_values(['senders_address', 'transaction_timestamp'], inplace=True)\n",
    "\n",
    "# Add a column for transaction count within the last 12 hours, initialized with 0\n",
    "df['trans_freq'] = 0\n",
    "\n",
    "# Initialize the dictionary to store transaction counts\n",
    "transaction_counts = defaultdict(list)\n",
    "\n",
    "# Loop over each row in the DataFrame\n",
    "for i in range(len(df)):\n",
    "    # Get the current user and transaction timestamp\n",
    "    current_user = df.iloc[i]['senders_address']\n",
    "    current_time = df.iloc[i]['transaction_timestamp']\n",
    "\n",
    "    # Define the 12-hour window start time\n",
    "    window_start = current_time - pd.Timedelta(hours=12)\n",
    "\n",
    "    # Store transaction timestamps for each user\n",
    "    transaction_counts[current_user].append(current_time)\n",
    "\n",
    "    # Keep only transactions within the 12-hour window\n",
    "    transaction_counts[current_user] = [ts for ts in transaction_counts[current_user] if ts >= window_start]\n",
    "\n",
    "    # Set the transaction frequency for the current row to the number of transactions within the time window\n",
    "    df.at[i, 'trans_freq'] = len(transaction_counts[current_user])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d67dce-a564-4fd1-8a9d-2065b214c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61692649-ebd6-4d63-a302-7d7f7afa00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to datetime and create a temporary column\n",
    "df['temp_timestamp'] = pd.to_datetime(df['transaction_timestamp'])\n",
    "\n",
    "# Set the launch dates for Layer 2 solutions\n",
    "optimism_launch = datetime(2021, 1, 16)\n",
    "arbitrum_launch = datetime(2021, 8, 31)\n",
    "\n",
    "# Add the 'layer2_availability' column\n",
    "df['layer2_availability'] = ((df['temp_timestamp'] >= arbitrum_launch) | (df['temp_timestamp'] >= optimism_launch)).astype(int)\n",
    "\n",
    "# Add the 'post_eip1559' column. \n",
    "# EIP-1559 went live on August 5, 2021\n",
    "eip1559_launch_date = datetime(2021, 8, 5)\n",
    "df['post_eip1559'] = (df['temp_timestamp'] >= eip1559_launch_date).astype(int)\n",
    "\n",
    "# Drop the temporary column\n",
    "df = df.drop('temp_timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb7baa-46fb-4e3e-a0f0-48bd7972a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd44fa-3a18-43f8-94d7-39f749cb6d8b",
   "metadata": {},
   "source": [
    "---\n",
    "1. **User transaction count**: The cumulative number of transactions made by the same user up to the current transaction.\n",
    "2. **Failed transactions**: the cumulative number of failed transactions by the same user up to the current transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57520029-c8ee-49ad-bb68-30d54a81c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that is 1 if the transaction failed and 0 otherwise\n",
    "df['is_failed'] = (df['transaction_status'] == 'False').astype(int)\n",
    "\n",
    "# Group by user address and calculate the cumulative count of transactions and failed transactions\n",
    "df = df.sort_values('transaction_timestamp')\n",
    "df['user_transaction_count'] = df.groupby('senders_address').cumcount() + 1\n",
    "df['failed_transactions'] = df.groupby('senders_address')['is_failed'].cumsum()\n",
    "\n",
    "# You can drop the 'is_failed' column if you no longer need it\n",
    "df = df.drop('is_failed', axis=1)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee14327-23a5-4ffa-a296-22e58a102fdf",
   "metadata": {},
   "source": [
    "In this script, we first created a new column `is_failed` that is 1 if the transaction failed and 0 otherwise. We then sorted the dataframe by the transaction timestamp to ensure transactions are processed in the order they occurred. \n",
    "\n",
    "Next, we grouped the dataframe by the sender's address and used `cumcount` to get the cumulative count of transactions for each user (we added 1 because `cumcount` starts from 0). We also used `cumsum` on the `is_failed` column to get the cumulative count of failed transactions for each user. \n",
    "\n",
    "Finally, we dropped the `is_failed` column as it's no longer needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c9adf-b010-49c8-b17f-412c7e1068e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ec8da-9779-419d-8478-4b3517aa50ff",
   "metadata": {},
   "source": [
    "---\n",
    "Derive User Address Age\n",
    "\n",
    "We are using 'eth.getTransactionCount' function from web3.py, which retrieves the number of transactions sent from an address up until a certain block. Using this function, we can find the block at which the address was first used.\n",
    "\n",
    "\n",
    "In the script below, for each transaction in the dataframe, the script scans blocks from the genesis block to the block of the transaction. If the address has made any transactions up to a block, it means that this block is the first usage block of the address, so it's saved to the `first_usage` dictionary. The `get_transaction_count` function is used to get the number of transactions sent from an address up to a certain block. After finding the first usage block of each address, the script adds a new column 'address_age' to the data, which represents the age of the address at the time of each transaction.\n",
    "\n",
    "The above solution is likely to be slow if you have a large number of unique addresses and transactions. It's because for each address, it scans blocks from the genesis block to the block of each transaction of the address. If you have a large number of unique addresses, the total number of blocks to scan can be enormous.\n",
    "\n",
    "To optimize the calculation of user address age and utilize a high-performance computing cluster, we could use parallel computing techniques. This involves dividing the computation task into smaller jobs that can be run simultaneously across multiple processors or nodes in the cluster.\n",
    "\n",
    "In Python, several libraries allow you to use parallel computing, such as `multiprocessing`, `concurrent.futures`, and `joblib`. Here's an example of how you could use the `concurrent.futures` library to parallelize the block scanning task. This script uses a `ThreadPoolExecutor` to run multiple block scanning tasks simultaneously, which should significantly speed up the process if you're running it on a machine with multiple CPU cores. However, the maximum number of concurrent tasks is limited by the number of CPU cores in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cf3ab-0492-4c76-9cbb-d14bb4a1e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the first usage block of an address\n",
    "def find_first_usage(address, transaction_block):\n",
    "    # Scan blocks from the start block to the transaction block\n",
    "    for block in range(start_block, transaction_block):\n",
    "        # If the address has made any transactions up to the block\n",
    "        if web3.eth.get_transaction_count(address, block):\n",
    "            return block\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c5c3f-0bf0-490f-9865-9dcb3a5e2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by block number\n",
    "df.sort_values(['block_number'], inplace=True)\n",
    "\n",
    "# Prepare a dictionary to store the first usage block of each address\n",
    "first_usage = {}\n",
    "\n",
    "# Define the range of blocks to scan\n",
    "start_block = 0  # This should be set to the genesis block\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # For each unique address in the data\n",
    "    for address in df['senders_address'].unique():\n",
    "        transaction_block = df[df['senders_address'] == address]['block_number'].min()\n",
    "        \n",
    "        # If the first usage block of the address has already been found, skip this address\n",
    "        if address in first_usage:\n",
    "            continue\n",
    "        \n",
    "        # Submit a new task to the executor\n",
    "        future = executor.submit(find_first_usage, address, transaction_block)\n",
    "        \n",
    "        # Store the Future object in the dictionary\n",
    "        first_usage[address] = future\n",
    "\n",
    "# Retrieve the results from the Future objects\n",
    "for address, future in first_usage.items():\n",
    "    first_usage[address] = future.result()\n",
    "\n",
    "# Add a new column 'address_age' to the data\n",
    "df['address_age'] = df['senders_address'].map(first_usage)\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df0f47-2c25-41bd-b611-7cb62a8e7711",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Time of the day variable**\n",
    "\n",
    "Due to the global nature of Ethereum transactions, the \"time of day\" variable could have different implications for users in different time zones, and this could indeed introduce complexity and potentially confounding effects into the analysis.\n",
    "\n",
    "However, there may still be value in including a \"time of day\" variable, even in a global context, for several reasons:\n",
    "\n",
    "1. **Network Effects**: Blockchain networks like Ethereum may experience periods of higher and lower congestion, which could align with certain times of day, despite global usage. For example, if a substantial proportion of Ethereum users are based in a particular region (say, North America or East Asia), then the network might be busier during the waking hours of that region.\n",
    "\n",
    "2. **Market Activity**: Cryptocurrency markets operate 24/7 and market activity (trading volume, price volatility, etc.) can vary significantly across different times of the day, potentially influencing user behavior. For example, users might be more likely to engage in DeFi transactions during periods of high market activity.\n",
    "\n",
    "3. **Behavioral Patterns**: Regardless of the global nature of Ethereum, there might be common daily behavioral patterns. For example, users might be more active during their daytime and less active during their nighttime, and these patterns could aggregate up to observable patterns in the data.\n",
    "\n",
    "However, given the global nature, it may be advisable to construct the \"time of day\" variable in a way that captures potential global effects. For example, you could split the day into fewer, larger chunks (like morning, afternoon, evening, and night), or analyze this variable carefully in your exploratory analysis to understand its distribution and potential impacts.\n",
    "\n",
    "Another approach might be to construct a variable that captures the \"local time of day\" for each transaction, assuming you can infer or have information about the geographic location of each user (which could introduce privacy issues and may not be feasible). But again, these are more complex and may not necessarily offer a better representation.\n",
    "\n",
    "Finally, including \"time of day\" in your initial model does not obligate you to keep it in your final model. If exploratory analysis or preliminary model results suggest it's not meaningful or is causing issues, you can always exclude it in later iterations of your modeling process.\n",
    "\n",
    "In conclusion, the \"time of day\" could potentially be a meaningful variable in your analysis, but its use and interpretation require careful consideration due to the global nature of Ethereum usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073c20-533b-4862-a2f6-3ac84973fb97",
   "metadata": {},
   "source": [
    "Given the global nature of Ethereum, a simple division of a day into distinct periods based on a specific time zone may not be the most representative. However, a potential solution could be to divide the day into a number of periods that are likely to capture significant changes in activity. For instance:\n",
    "\n",
    "1. **Daytime**: 06:00 to 17:59\n",
    "2. **Evening**: 18:00 to 21:59\n",
    "3. **Night**: 22:00 to 05:59\n",
    "\n",
    "This division attempts to capture typical working hours (daytime), after-work hours (evening), and sleeping hours (night). Of course, given the global nature of the network, these periods won't align perfectly with these times for all users, but they might serve as a useful approximation.\n",
    "\n",
    "If it's possible to incorporate additional information, such as the geographic distribution of Ethereum users or the times of day that tend to see the most network activity, this could be used to refine these periods further.\n",
    "\n",
    "You can then create a new variable, \"TimeOfDay\", in your dataset by mapping each transaction timestamp to one of these periods.\n",
    "\n",
    "Here's a Python script to do that assuming your timestamp is in the form 'YYYY-MM-DD HH:MM:SS' and in UTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11eef1-b935-4cef-93b8-89357704b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_time_of_day(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    if 6 <= hour < 18:\n",
    "        return 'Daytime'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['time_of_day'] = df['transaction_timestamp'].apply(assign_time_of_day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
