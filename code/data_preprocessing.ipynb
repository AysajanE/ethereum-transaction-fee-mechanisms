{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fcb88-e91f-466a-bd78-bd939066170d",
   "metadata": {},
   "source": [
    "# Transaction Data Collection from the Ethereum Blockchain\n",
    "\n",
    "## EIP-1559\n",
    "- Date: August 5, 2021\n",
    "- Block number: 12,965,000\n",
    "- [Ethereum JSON-RPC Specification](https://ethereum.github.io/execution-apis/api-documentation/)\n",
    "- [JSON RPC API](https://ethereum.org/en/developers/docs/apis/json-rpc/)\n",
    "- [EIP-1559 Analysis Arxiv](https://github.com/SciEcon/EIP1559)\n",
    "\n",
    "## Layer 2 Solutions Launch Dates\n",
    "Source: [L2BEAT](https://l2beat.com/scaling/tvl)\n",
    "1. Optimism is live on: January 16, 2021\n",
    "2. Arbitrum is live on: August 31, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148795ae-d2a2-45d3-9518-7bca8dbb931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask\n",
    "# !pip install pyspark\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d46b85a-a478-4e4c-b1f5-0ccc63b33ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from collections import defaultdict\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, unix_timestamp, round, to_timestamp, lit, when, sum, row_number, date_format, to_date, from_unixtime\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600db6e9-0b3a-4705-975d-7b3c6fc6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Convert a date string to a Unix timestamp.\n",
    "    \n",
    "    Args:\n",
    "        date_string (str): The date string in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        int: The Unix timestamp corresponding to the date string.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    return int(dt.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150d89b-a6e3-465f-88c6-ea58ee3e18b1",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Given the large size of your data, traditional Python tools like Pandas might not be able to handle it efficiently due to memory constraints. Fortunately, there are tools and libraries built specifically for handling larger-than-memory datasets, such as Dask and Apache Spark.\n",
    "\n",
    "1. **Dask**: Dask is a flexible library for parallel computing in Python that's built on top of existing Python APIs and data structures, like NumPy arrays and Pandas DataFrames. It can handle larger-than-memory computations by breaking them down into smaller tasks, executing these tasks in parallel and combining the results.\n",
    "\n",
    "\n",
    "2. **Apache Spark**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b997b5-b406-4654-b1d1-b1be983287f7",
   "metadata": {},
   "source": [
    "---\n",
    "### Use Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe438c-efe2-4650-a7ff-00f5d5b57602",
   "metadata": {},
   "source": [
    "One approach I will explore is using Apache Spark. The details are provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e363e7-a5cc-477a-9fd3-a04de717d497",
   "metadata": {},
   "source": [
    "Sure! Here is a step-by-step guide using PySpark.\n",
    "\n",
    "1. **Setup Apache Spark:** The first step is to set up Apache Spark. Since Spark is written in Scala and runs on the Java Virtual Machine (JVM), you will need to have Java installed on your machine. You can download Apache Spark from the official website and then extract it to a directory of your choice.\n",
    "\n",
    "2. **Install PySpark:** The next step is to install PySpark, which is the Python library for Apache Spark. You can do this with pip:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "3. **Load the Data:** Now that you have Spark set up, you can start a SparkSession and use it to read in your data. For your CSV file, you would do this:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ethereum Transaction Frequency\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.csv(\"../data/eth_transaction_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Print the schema to verify it was loaded correctly\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "4. **Preprocess the Data:** The transaction_timestamp column needs to be converted to a TimestampType, and then we need to create a window over each sender's transactions that includes only the transactions from the last 12 hours:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Convert transaction_timestamp column to TimestampType\n",
    "df = df.withColumn(\"transaction_timestamp\", (col(\"transaction_timestamp\").cast(TimestampType())))\n",
    "```\n",
    "\n",
    "5. **Calculate Transaction Frequencies:** Now we can calculate the transaction frequency for each 12-hour window:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Define a window spec\n",
    "window_spec = Window.partitionBy(\"senders_address\").orderBy(\"transaction_timestamp\").rangeBetween(-12*60*60, 0)\n",
    "\n",
    "# Calculate the transaction frequency\n",
    "df = df.withColumn(\"trans_freq\", count(\"transaction_identifier\").over(window_spec))\n",
    "```\n",
    "\n",
    "6. **Save the Results:** Finally, save the results back to a CSV file:\n",
    "\n",
    "```python\n",
    "# Save the results to a new CSV file\n",
    "df.write.csv(\"processed_merged_eth_transaction_data.csv\", header=True)\n",
    "```\n",
    "\n",
    "7. **Close the Spark Session:** Don't forget to close the SparkSession when you're done:\n",
    "\n",
    "```python\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "Please note that the SparkSession is configured with default parameters, so it will use all available cores and as much memory as it can. If you're running other processes on the machine, you might want to configure the SparkSession to use less resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638bb54-bed4-4ddb-b1b7-8aa02cb64063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transaction_frequency(df):\n",
    "    \"\"\"\n",
    "    This function calculates the transaction frequency for each sender's address\n",
    "    within a 12-hour window.\n",
    "\n",
    "    Parameters:\n",
    "    df (pyspark.sql.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    df (pyspark.sql.DataFrame): The processed DataFrame with an additional column for transaction frequency.\n",
    "    \"\"\"\n",
    "    # Convert transaction_timestamp column to TimestampType\n",
    "    df = df.withColumn(\"transaction_timestamp\", (col(\"transaction_timestamp\").cast(TimestampType())))\n",
    "\n",
    "    # Define a window spec\n",
    "    window_spec = Window.partitionBy(\"senders_address\").orderBy(\"transaction_timestamp\").rangeBetween(-12*60*60, 0)\n",
    "\n",
    "    # Calculate the transaction frequency\n",
    "    df = df.withColumn(\"trans_freq\", count(\"transaction_identifier\").over(window_spec))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677a41d-6e95-45e1-8003-dc59f1d26730",
   "metadata": {},
   "source": [
    "### Preliminary Data Analysis using Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7912e11d-c07a-4480-ab95-0b0d1d8df793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/16 11:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ethereum Transaction Frequency\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../data/eth_transaction_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227fec8-765a-45d5-9b3a-7dbf59b1a3ee",
   "metadata": {},
   "source": [
    "If you want to get basic information about the DataFrame `df` read by PySpark, you can use the following methods:\n",
    "\n",
    "1. **Print Schema**: The `printSchema()` method prints the types of columns in the DataFrame.\n",
    "\n",
    "```python\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "2. **Show Rows**: The `show()` method displays the first N rows. Default is 20 rows.\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "```\n",
    "\n",
    "Or to show the first 5 rows:\n",
    "\n",
    "```python\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "3. **Count Rows**: The `count()` method returns the number of rows.\n",
    "\n",
    "```python\n",
    "print(\"Number of rows: \", df.count())\n",
    "```\n",
    "\n",
    "4. **Column Names**: The `columns` property returns the column names.\n",
    "\n",
    "```python\n",
    "print(\"Column names: \", df.columns)\n",
    "```\n",
    "\n",
    "5. **Summary Statistics**: The `describe()` method computes statistics for numeric and string columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns.\n",
    "\n",
    "```python\n",
    "df.describe().show()\n",
    "```\n",
    "\n",
    "Note: `show()` method is used here to view the summary. The `describe()` method itself returns another DataFrame representing the summary information, which is not human-readable until you call `show()`.\n",
    "\n",
    "6. **Number of Unique Values per Column**: This is a bit more involved, you'd need to use the `distinct()` and `count()` methods for each column.\n",
    "\n",
    "For example, for a column 'senders_address', you would do:\n",
    "\n",
    "```python\n",
    "df.select('senders_address').distinct().count()\n",
    "```\n",
    "\n",
    "You need to repeat this for each column you're interested in.\n",
    "\n",
    "Remember that Apache Spark operations are lazily evaluated, meaning that they don't compute the results right away - they just remember the transformations applied to some base dataset (e.g., a file). The transformations are only computed when an action requires a result to be returned to the driver program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664f2d7e-adb2-4531-bf8a-2665d4df4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- hash: string (nullable = true)\n",
      " |-- nonce: integer (nullable = true)\n",
      " |-- transaction_index: integer (nullable = true)\n",
      " |-- from_address: string (nullable = true)\n",
      " |-- to_address: string (nullable = true)\n",
      " |-- value: decimal(24,0) (nullable = true)\n",
      " |-- gas: integer (nullable = true)\n",
      " |-- gas_price: long (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      " |-- receipt_cumulative_gas_used: integer (nullable = true)\n",
      " |-- receipt_gas_used: integer (nullable = true)\n",
      " |-- receipt_contract_address: string (nullable = true)\n",
      " |-- receipt_root: string (nullable = true)\n",
      " |-- receipt_status: integer (nullable = true)\n",
      " |-- block_timestamp: timestamp (nullable = true)\n",
      " |-- block_number: integer (nullable = true)\n",
      " |-- block_hash: string (nullable = true)\n",
      " |-- max_fee_per_gas: double (nullable = true)\n",
      " |-- max_priority_fee_per_gas: double (nullable = true)\n",
      " |-- transaction_type: double (nullable = true)\n",
      " |-- receipt_effective_gas_price: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the types of columns in the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cd42be-4c61-417e-a4ea-ed319e467519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+-----------------+--------------------+--------------------+------------------+-------+------------+--------------------+---------------------------+----------------+------------------------+------------+--------------+-------------------+------------+--------------------+----------------+------------------------+----------------+---------------------------+\n",
      "|_c0|                hash|  nonce|transaction_index|        from_address|          to_address|             value|    gas|   gas_price|               input|receipt_cumulative_gas_used|receipt_gas_used|receipt_contract_address|receipt_root|receipt_status|    block_timestamp|block_number|          block_hash| max_fee_per_gas|max_priority_fee_per_gas|transaction_type|receipt_effective_gas_price|\n",
      "+---+--------------------+-------+-----------------+--------------------+--------------------+------------------+-------+------------+--------------------+---------------------------+----------------+------------------------+------------+--------------+-------------------+------------+--------------------+----------------+------------------------+----------------+---------------------------+\n",
      "|  0|0xd9e71df34c1391f...|     51|              170|0x37d682aef85a1d7...|0x7be8076f4ea4a4a...|200000000000000000| 328192|107526096812|0xab834bab0000000...|                   14468255|          237914|                    null|        null|             1|2021-08-26 19:41:55|    13104057|0x3f2f67cb296e7c4...|1.34571253504E11|           1.665944009E9|             2.0|               107526096812|\n",
      "|  1|0xc99f621749c2512...|3144541|               68|0x2faf487a4414fe7...|0x50d1c9771902476...|                 0|  68608| 94907631600|0x23b872dd0000000...|                    2790841|           37374|                    null|        null|             1|2022-02-04 03:09:37|    14138347|0x6c4f3cd861b37b4...|          1.0E12|                  9.63E9|             2.0|                94907631600|\n",
      "|  2|0xf487c73750480e9...|2259982|               25|0x2faf487a4414fe7...|0x50d1c9771902476...|                 0|  68608|145140999371|0x23b872dd0000000...|                    2330999|           37374|                    null|        null|             1|2021-10-15 01:38:34|    13420802|0x09b3adc3731d708...|          1.0E12|               1.4245E10|             2.0|               145140999371|\n",
      "|  3|0x5afdcb25e7dcd09...|2360568|              104|0x2faf487a4414fe7...|0x50d1c9771902476...|                 0|  68608|138664661599|0x23b872dd0000000...|                    5221854|           37374|                    null|        null|             1|2021-10-28 06:51:15|    13505282|0xe98c2a78b626556...|          1.0E12|               1.2945E10|             2.0|               138664661599|\n",
      "|  4|0x01864fc4150095d...|2266430|               53|0x2faf487a4414fe7...|0x50d1c9771902476...|                 0|  68608|136917930727|0x23b872dd0000000...|                    2964702|           37374|                    null|        null|             1|2021-10-15 19:51:32|    13425656|0x5da11ec7649229f...|          1.0E12|               1.4375E10|             2.0|               136917930727|\n",
      "|  5|0x4dcb6790e62f03f...|2039195|               24|0x2faf487a4414fe7...|0x50d1c9771902476...|                 0|  68608| 73218396684|0x23b872dd0000000...|                    3760072|           37374|                    null|        null|             1|2021-09-12 03:06:46|    13209488|0x9374b6caffd2d8e...|       1.0426E12|               2.9065E10|             2.0|                73218396684|\n",
      "|  6|0xfdb747a7da7ef64...|2032687|              322|0x2faf487a4414fe7...|0x50d1c9771902476...|                 0|  68608| 53148522348|0x23b872dd0000000...|                   20166027|           37374|                    null|        null|             1|2021-09-11 05:56:09|    13203784|0x068439ef7c7b8d2...|          1.0E12|                  7.29E9|             2.0|                53148522348|\n",
      "|  7|0xcb874eb2cc0c181...|    777|               66|0x24d9be8c2589c98...|0xbdc2a52811529aa...|                 0| 135936| 57346002875|          0x290e4544|                    3955294|           90435|                    null|        null|             1|2021-12-19 09:32:03|    13836153|0x1397d179b2fa538...| 5.7346002875E10|         5.7346002875E10|             2.0|                57346002875|\n",
      "|  8|0xc62bbbd4ac4f4b3...|    277|               32|0x6d3bd5039a69ed3...|0xab1391d8182f176...|                 0| 726784| 48328917514|0xfebbf3fa0000000...|                    1888119|          402735|                    null|        null|             1|2021-12-17 08:45:25|    13822951|0xdbcd1d9a6588767...| 4.8328917514E10|         4.8328917514E10|             2.0|                48328917514|\n",
      "|  9|0x1184f39da16f568...|    144|              253|0x21a3a96171564f5...|0x21a3a96171564f5...|                 0|1120000|105004032427|                  0x|                   16893690|           21000|                    null|        null|             1|2021-11-23 07:58:36|    13670976|0xcab20867cb83377...|          5.5E11|                  2.42E9|             2.0|               105004032427|\n",
      "+---+--------------------+-------+-----------------+--------------------+--------------------+------------------+-------+------------+--------------------+---------------------------+----------------+------------------------+------------+--------------+-------------------+------------+--------------------+----------------+------------------------+----------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 11:43:40 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , hash, nonce, transaction_index, from_address, to_address, value, gas, gas_price, input, receipt_cumulative_gas_used, receipt_gas_used, receipt_contract_address, receipt_root, receipt_status, block_timestamp, block_number, block_hash, max_fee_per_gas, max_priority_fee_per_gas, transaction_type, receipt_effective_gas_price\n",
      " Schema: _c0, hash, nonce, transaction_index, from_address, to_address, value, gas, gas_price, input, receipt_cumulative_gas_used, receipt_gas_used, receipt_contract_address, receipt_root, receipt_status, block_timestamp, block_number, block_hash, max_fee_per_gas, max_priority_fee_per_gas, transaction_type, receipt_effective_gas_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/aeziz-local/Research/@2023/blockchain-JOM-special-issue-Sep30/ethereum-transaction-fee-mechanisms/data/eth_transaction_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Display the first N rows. Default is 20 rows.\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76446f2f-c101-4ae2-9c1e-744bd8fc87ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================================>(2628 + 3) / 2631]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  461835216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count number of rows\n",
    "print(\"Number of rows: \", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeb3eec-a76d-43e5-ab85-dbd43a729264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['_c0', 'hash', 'nonce', 'transaction_index', 'from_address', 'to_address', 'value', 'gas', 'gas_price', 'input', 'receipt_cumulative_gas_used', 'receipt_gas_used', 'receipt_contract_address', 'receipt_root', 'receipt_status', 'block_timestamp', 'block_number', 'block_hash', 'max_fee_per_gas', 'max_priority_fee_per_gas', 'transaction_type', 'receipt_effective_gas_price']\n"
     ]
    }
   ],
   "source": [
    "# Return the column names\n",
    "print(\"Column names: \", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15579dee-68eb-4b33-bb37-0d1b99475024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086b843-6929-4068-81ac-a244c914c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_transaction_frequency(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61692649-ebd6-4d63-a302-7d7f7afa00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to datetime and create a temporary column\n",
    "df['temp_timestamp'] = pd.to_datetime(df['transaction_timestamp'])\n",
    "\n",
    "# Set the launch dates for Layer 2 solutions\n",
    "optimism_launch = datetime(2021, 1, 16)\n",
    "arbitrum_launch = datetime(2021, 8, 31)\n",
    "\n",
    "# Add the 'layer2_availability' column\n",
    "df['layer2_availability'] = ((df['temp_timestamp'] >= arbitrum_launch) | (df['temp_timestamp'] >= optimism_launch)).astype(int)\n",
    "\n",
    "# Add the 'post_eip1559' column. \n",
    "# EIP-1559 went live on August 5, 2021\n",
    "eip1559_launch_date = datetime(2021, 8, 5)\n",
    "df['post_eip1559'] = (df['temp_timestamp'] >= eip1559_launch_date).astype(int)\n",
    "\n",
    "# Drop the temporary column\n",
    "df = df.drop('temp_timestamp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf33df-18f2-42dc-8687-6a4798e9fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to datetime \n",
    "df = df.withColumn('transaction_timestamp', to_timestamp(col('transaction_timestamp')))\n",
    "\n",
    "# Set the launch dates for Layer 2 solutions\n",
    "optimism_launch = lit(datetime(2021, 1, 16))\n",
    "arbitrum_launch = lit(datetime(2021, 8, 31))\n",
    "\n",
    "# Add the 'layer2_availability' column\n",
    "df = df.withColumn(\n",
    "    'layer2_availability',\n",
    "    when((col('transaction_timestamp') >= arbitrum_launch) | (col('transaction_timestamp') >= optimism_launch), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Add the 'post_eip1559' column. \n",
    "# EIP-1559 went live on August 5, 2021\n",
    "eip1559_launch_date = lit(datetime(2021, 8, 5))\n",
    "df = df.withColumn(\n",
    "    'post_eip1559',\n",
    "    when(col('transaction_timestamp') >= eip1559_launch_date, 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb7baa-46fb-4e3e-a0f0-48bd7972a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd44fa-3a18-43f8-94d7-39f749cb6d8b",
   "metadata": {},
   "source": [
    "---\n",
    "1. **User transaction count**: The cumulative number of transactions made by the same user up to the current transaction.\n",
    "2. **Failed transactions**: the cumulative number of failed transactions by the same user up to the current transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56eebe-80b6-4a4e-ab0e-959e22230355",
   "metadata": {},
   "source": [
    "### Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805727f-5082-4f01-aaec-05c85b02d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that is 1 if the transaction failed and 0 otherwise\n",
    "df = df.withColumn(\n",
    "    'is_failed',\n",
    "    when(col('transaction_status') == 'False', 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Define a window partitioned by senders_address and ordered by transaction_timestamp\n",
    "window = Window.partitionBy(\"senders_address\").orderBy(\"transaction_timestamp\")\n",
    "\n",
    "# Group by user address and calculate the cumulative count of transactions and failed transactions\n",
    "df = df.withColumn(\"user_transaction_count\", row_number().over(window))\n",
    "df = df.withColumn(\"failed_transactions\", sum(\"is_failed\").over(window))\n",
    "\n",
    "# You can drop the 'is_failed' column if you no longer need it\n",
    "df = df.drop(\"is_failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944a9c4-e78b-4ed0-939e-87b39b6d68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18bab7-328d-4d94-97e0-561099d264f7",
   "metadata": {},
   "source": [
    "### Python Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57520029-c8ee-49ad-bb68-30d54a81c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that is 1 if the transaction failed and 0 otherwise\n",
    "df['is_failed'] = (df['transaction_status'] == 'False').astype(int)\n",
    "\n",
    "# Group by user address and calculate the cumulative count of transactions and failed transactions\n",
    "df = df.sort_values('transaction_timestamp')\n",
    "df['user_transaction_count'] = df.groupby('senders_address').cumcount() + 1\n",
    "df['failed_transactions'] = df.groupby('senders_address')['is_failed'].cumsum()\n",
    "\n",
    "# You can drop the 'is_failed' column if you no longer need it\n",
    "df = df.drop('is_failed', axis=1)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee14327-23a5-4ffa-a296-22e58a102fdf",
   "metadata": {},
   "source": [
    "In this script, we first created a new column `is_failed` that is 1 if the transaction failed and 0 otherwise. We then sorted the dataframe by the transaction timestamp to ensure transactions are processed in the order they occurred. \n",
    "\n",
    "Next, we grouped the dataframe by the sender's address and used `cumcount` to get the cumulative count of transactions for each user (we added 1 because `cumcount` starts from 0). We also used `cumsum` on the `is_failed` column to get the cumulative count of failed transactions for each user. \n",
    "\n",
    "Finally, we dropped the `is_failed` column as it's no longer needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c9adf-b010-49c8-b17f-412c7e1068e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ec8da-9779-419d-8478-4b3517aa50ff",
   "metadata": {},
   "source": [
    "---\n",
    "Derive User Address Age\n",
    "\n",
    "We are using 'eth.getTransactionCount' function from web3.py, which retrieves the number of transactions sent from an address up until a certain block. Using this function, we can find the block at which the address was first used.\n",
    "\n",
    "\n",
    "In the script below, for each transaction in the dataframe, the script scans blocks from the genesis block to the block of the transaction. If the address has made any transactions up to a block, it means that this block is the first usage block of the address, so it's saved to the `first_usage` dictionary. The `get_transaction_count` function is used to get the number of transactions sent from an address up to a certain block. After finding the first usage block of each address, the script adds a new column 'address_age' to the data, which represents the age of the address at the time of each transaction.\n",
    "\n",
    "The above solution is likely to be slow if you have a large number of unique addresses and transactions. It's because for each address, it scans blocks from the genesis block to the block of each transaction of the address. If you have a large number of unique addresses, the total number of blocks to scan can be enormous.\n",
    "\n",
    "To optimize the calculation of user address age and utilize a high-performance computing cluster, we could use parallel computing techniques. This involves dividing the computation task into smaller jobs that can be run simultaneously across multiple processors or nodes in the cluster.\n",
    "\n",
    "In Python, several libraries allow you to use parallel computing, such as `multiprocessing`, `concurrent.futures`, and `joblib`. Here's an example of how you could use the `concurrent.futures` library to parallelize the block scanning task. This script uses a `ThreadPoolExecutor` to run multiple block scanning tasks simultaneously, which should significantly speed up the process if you're running it on a machine with multiple CPU cores. However, the maximum number of concurrent tasks is limited by the number of CPU cores in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cf3ab-0492-4c76-9cbb-d14bb4a1e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the first usage block of an address\n",
    "def find_first_usage(address, transaction_block):\n",
    "    # Scan blocks from the start block to the transaction block\n",
    "    for block in range(start_block, transaction_block):\n",
    "        # If the address has made any transactions up to the block\n",
    "        if web3.eth.get_transaction_count(address, block):\n",
    "            return block\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c5c3f-0bf0-490f-9865-9dcb3a5e2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by block number\n",
    "df.sort_values(['block_number'], inplace=True)\n",
    "\n",
    "# Prepare a dictionary to store the first usage block of each address\n",
    "first_usage = {}\n",
    "\n",
    "# Define the range of blocks to scan\n",
    "start_block = 0  # This should be set to the genesis block\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # For each unique address in the data\n",
    "    for address in df['senders_address'].unique():\n",
    "        transaction_block = df[df['senders_address'] == address]['block_number'].min()\n",
    "        \n",
    "        # If the first usage block of the address has already been found, skip this address\n",
    "        if address in first_usage:\n",
    "            continue\n",
    "        \n",
    "        # Submit a new task to the executor\n",
    "        future = executor.submit(find_first_usage, address, transaction_block)\n",
    "        \n",
    "        # Store the Future object in the dictionary\n",
    "        first_usage[address] = future\n",
    "\n",
    "# Retrieve the results from the Future objects\n",
    "for address, future in first_usage.items():\n",
    "    first_usage[address] = future.result()\n",
    "\n",
    "# Add a new column 'address_age' to the data\n",
    "df['address_age'] = df['senders_address'].map(first_usage)\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df0f47-2c25-41bd-b611-7cb62a8e7711",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Time of the day variable**\n",
    "\n",
    "Due to the global nature of Ethereum transactions, the \"time of day\" variable could have different implications for users in different time zones, and this could indeed introduce complexity and potentially confounding effects into the analysis.\n",
    "\n",
    "However, there may still be value in including a \"time of day\" variable, even in a global context, for several reasons:\n",
    "\n",
    "1. **Network Effects**: Blockchain networks like Ethereum may experience periods of higher and lower congestion, which could align with certain times of day, despite global usage. For example, if a substantial proportion of Ethereum users are based in a particular region (say, North America or East Asia), then the network might be busier during the waking hours of that region.\n",
    "\n",
    "2. **Market Activity**: Cryptocurrency markets operate 24/7 and market activity (trading volume, price volatility, etc.) can vary significantly across different times of the day, potentially influencing user behavior. For example, users might be more likely to engage in DeFi transactions during periods of high market activity.\n",
    "\n",
    "3. **Behavioral Patterns**: Regardless of the global nature of Ethereum, there might be common daily behavioral patterns. For example, users might be more active during their daytime and less active during their nighttime, and these patterns could aggregate up to observable patterns in the data.\n",
    "\n",
    "However, given the global nature, it may be advisable to construct the \"time of day\" variable in a way that captures potential global effects. For example, you could split the day into fewer, larger chunks (like morning, afternoon, evening, and night), or analyze this variable carefully in your exploratory analysis to understand its distribution and potential impacts.\n",
    "\n",
    "Another approach might be to construct a variable that captures the \"local time of day\" for each transaction, assuming you can infer or have information about the geographic location of each user (which could introduce privacy issues and may not be feasible). But again, these are more complex and may not necessarily offer a better representation.\n",
    "\n",
    "Finally, including \"time of day\" in your initial model does not obligate you to keep it in your final model. If exploratory analysis or preliminary model results suggest it's not meaningful or is causing issues, you can always exclude it in later iterations of your modeling process.\n",
    "\n",
    "In conclusion, the \"time of day\" could potentially be a meaningful variable in your analysis, but its use and interpretation require careful consideration due to the global nature of Ethereum usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41073c20-533b-4862-a2f6-3ac84973fb97",
   "metadata": {},
   "source": [
    "Given the global nature of Ethereum, a simple division of a day into distinct periods based on a specific time zone may not be the most representative. However, a potential solution could be to divide the day into a number of periods that are likely to capture significant changes in activity. For instance:\n",
    "\n",
    "1. **Daytime**: 06:00 to 17:59\n",
    "2. **Evening**: 18:00 to 21:59\n",
    "3. **Night**: 22:00 to 05:59\n",
    "\n",
    "This division attempts to capture typical working hours (daytime), after-work hours (evening), and sleeping hours (night). Of course, given the global nature of the network, these periods won't align perfectly with these times for all users, but they might serve as a useful approximation.\n",
    "\n",
    "If it's possible to incorporate additional information, such as the geographic distribution of Ethereum users or the times of day that tend to see the most network activity, this could be used to refine these periods further.\n",
    "\n",
    "You can then create a new variable, \"TimeOfDay\", in your dataset by mapping each transaction timestamp to one of these periods.\n",
    "\n",
    "Here's a Python script to do that assuming your timestamp is in the form 'YYYY-MM-DD HH:MM:SS' and in UTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11eef1-b935-4cef-93b8-89357704b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_time_of_day(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    if 6 <= hour < 18:\n",
    "        return 'Daytime'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['time_of_day'] = df['transaction_timestamp'].apply(assign_time_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a82c6-ba16-4033-a5df-e6037c8dba6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24cb85f-1e46-4aff-895a-e40689ad9469",
   "metadata": {},
   "source": [
    "Below is a step-by-step guide on preliminary data analysis and visualization using PySpark and its integrations.\n",
    "\n",
    "Please note that for visualizing data directly from Spark DataFrame, we would need to convert it to Pandas DataFrame. Be careful with this conversion as it might lead to out-of-memory errors for large datasets. For large datasets, it's recommended to perform aggregations or sampling in Spark before converting to Pandas for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598827b-cbcf-4b15-abec-b1a082f7b819",
   "metadata": {},
   "source": [
    "1. **Inspect the first few records to understand the data better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0461002-0731-4093-94fe-433138c3f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4a40b-db03-4a48-b723-b55c23466572",
   "metadata": {},
   "source": [
    "2. **Descriptive statistics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e36465-29b6-460a-ae71-69ad567f146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b83ff2-05db-4363-a34a-b04cafdd721a",
   "metadata": {},
   "source": [
    "3. **Count the number of distinct senders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a583836-1bd3-4052-9926-4e0993fb0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_senders = df.select(\"senders_address\").distinct().count()\n",
    "print(f\"The number of distinct senders: {num_senders}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c91c2-d108-46b7-a186-e68ad15f1811",
   "metadata": {},
   "source": [
    "4. **The number of transactions per transaction type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd7a1e-f82f-417a-8741-b36c63759797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"transaction_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a8666-66f9-46a9-ac8d-ef547a4e9f9d",
   "metadata": {},
   "source": [
    "5. **Distribution of transaction fee**\n",
    "\n",
    "Convert to Pandas DataFrame and use seaborn to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5010e-2791-442c-aa6c-b4613f1d9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.select(\"transaction_fee\").toPandas()\n",
    "sns.displot(pdf, x=\"transaction_fee\", kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecdde3-e779-4d1f-80c5-90fa2a6b67db",
   "metadata": {},
   "source": [
    "6. **Relationship between gas price and transaction fee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c588224-f397-4060-a864-9f7bc304b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.select(\"gas_price\", \"transaction_fee\").toPandas()\n",
    "sns.scatterplot(data=pdf, x=\"gas_price\", y=\"transaction_fee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59553c-5377-4f4b-b735-501f704efb43",
   "metadata": {},
   "source": [
    "7. **Count of transactions per EIP-1559 type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4459cc-6359-4fe4-bd7c-02c6a0d26a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"transaction_eip-1559_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449579ef-7968-4ad0-ab27-5b054092e1ee",
   "metadata": {},
   "source": [
    "8. **Failed transactions vs successful transactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba874b8-54d8-4f15-a7f1-7d63ab670bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"transaction_status\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24733081-2824-4183-8f2d-459e0c61236d",
   "metadata": {},
   "source": [
    "9. **Transactions before and after EIP1559**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbae4a-3ecc-40d6-b720-4f5529cd98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"post_eip1559\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031d495-93d0-42c1-9689-ba6987208bae",
   "metadata": {},
   "source": [
    "10. **Distribution of the number of transactions by users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8dc96-e59b-4c6e-ae0e-00abcd2925b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.select(\"user_transaction_count\").toPandas()\n",
    "sns.displot(pdf, x=\"user_transaction_count\", kde=True)\n",
    "plt.xlim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958f9c1-bcee-456a-b472-969e242d47f2",
   "metadata": {},
   "source": [
    "## More Detailed Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067ad8a-e493-47eb-abd5-a4dc58985138",
   "metadata": {},
   "source": [
    "Alright, let's deep dive into more detailed analysis of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806ac4b-2e0b-4bc7-a622-1dd6bc421577",
   "metadata": {},
   "source": [
    "1. **Detailed transactions over time**\n",
    "\n",
    "We'll create a time series plot to observe the total number of transactions over time. For better granularity, we'll resample the data to daily frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b630c56-cbc0-469e-89ce-b406a7837f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an hour column from timestamp\n",
    "df = df.withColumn('hour', date_format('transaction_timestamp', 'yyyy-MM-dd HH'))\n",
    "\n",
    "# Count transactions per hour\n",
    "df_per_hour = df.groupBy('hour').count()\n",
    "\n",
    "# Convert to pandas dataframe for plotting\n",
    "df_per_hour = df_per_hour.toPandas()\n",
    "\n",
    "# Convert 'hour' back to datetime and set as index for easy plotting\n",
    "df_per_hour['hour'] = pd.to_datetime(df_per_hour['hour'], format='%Y-%m-%d %H')\n",
    "df_per_hour.set_index('hour', inplace=True)\n",
    "\n",
    "# Plot\n",
    "df_per_hour.plot(figsize=(10,5))\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.title('Hourly Transactions Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127e1d0-4737-4ecf-99b5-e19f82de011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date column from timestamp\n",
    "df = df.withColumn('date', to_date('transaction_timestamp'))\n",
    "\n",
    "# Count transactions per day\n",
    "df_per_day = df.groupBy('date').count()\n",
    "\n",
    "# Convert to pandas dataframe for plotting\n",
    "df_per_day = df_per_day.toPandas()\n",
    "\n",
    "# Set date as index for easy plotting\n",
    "df_per_day.set_index('date', inplace=True)\n",
    "\n",
    "# Plot\n",
    "df_per_day.plot(figsize=(10,5))\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.title('Daily Transactions Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5c3f3-0803-470c-97ea-3fe5837bb025",
   "metadata": {},
   "source": [
    "2. **Correlation Matrix**\n",
    "\n",
    "A correlation matrix gives you an idea of how different variables interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efd582-acad-46a7-abe6-db39a363176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "num_df = df.select(\"gas_price\", \"transaction_fee\", \"user_transaction_count\", \"failed_transactions\")\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "num_pdf = num_df.toPandas()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = num_pdf.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df45fa-e3a0-4e20-a021-af84cbd084b9",
   "metadata": {},
   "source": [
    "3. **Gas price over time**\n",
    "\n",
    "Let's visualize how gas price has changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa13a13-37e3-4a2a-aa79-400df9beba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average gas price per hour\n",
    "df_gas_price = df.groupBy('hour').agg({\"gas_price\": \"avg\"})\n",
    "\n",
    "# Convert to pandas dataframe for plotting\n",
    "df_gas_price = df_gas_price.toPandas()\n",
    "\n",
    "# Convert 'hour' back to datetime and set as index for easy plotting\n",
    "df_gas_price['hour'] = pd.to_datetime(df_gas_price['hour'], format='%Y-%m-%d %H')\n",
    "df_gas_price.set_index('hour', inplace=True)\n",
    "\n",
    "# Plot\n",
    "df_gas_price.plot(figsize=(10,5))\n",
    "plt.ylabel('Average Gas Price')\n",
    "plt.title('Hourly Average Gas Price Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffb640-afee-4524-aa0f-7e8357acce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily average gas price\n",
    "df_gas_price = df.groupBy('date').agg({\"gas_price\": \"avg\"})\n",
    "\n",
    "# Convert to pandas dataframe for plotting\n",
    "df_gas_price = df_gas_price.toPandas()\n",
    "\n",
    "# Set date as index for easy plotting\n",
    "df_gas_price.set_index('date', inplace=True)\n",
    "\n",
    "# Plot\n",
    "df_gas_price.plot(figsize=(10,5))\n",
    "plt.ylabel('Average Gas Price')\n",
    "plt.title('Gas Price Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932dae88-5f44-4670-9191-a0461eef2c92",
   "metadata": {},
   "source": [
    "4. **Boxplot of transaction fees by transaction type**\n",
    "\n",
    "Boxplots are great for visualizing statistical measures (median, quartiles etc.) across different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff34011-ddbd-415f-bb14-2ae95948440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select necessary columns\n",
    "df_fee_type = df.select('transaction_fee', 'transaction_type')\n",
    "\n",
    "# Convert to pandas dataframe for plotting\n",
    "df_fee_type = df_fee_type.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='transaction_type', y='transaction_fee', data=df_fee_type)\n",
    "plt.title('Transaction Fees by Transaction Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db83924-5f9e-4b50-955c-7f1e54717fc7",
   "metadata": {},
   "source": [
    "5. **Number of Transactions Before and After EIP1559**\n",
    "\n",
    "We can visualize the number of transactions before and after the implementation of EIP1559 to observe if it had any effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e147-c9d1-4fb4-8a1b-1fd30cdfd5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by post_eip1559 and count transactions\n",
    "df_eip1559 = df.groupBy('post_eip1559').count()\n",
    "\n",
    "# Convert to pandas dataframe for plotting\n",
    "df_eip1559 = df_eip1559.toPandas()\n",
    "\n",
    "# Plot\n",
    "df_eip1559.plot(kind='bar', x='post_eip1559', y='count')\n",
    "plt.title('Number of Transactions Before and After EIP1559')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(ticks=[0, 1], labels=['Before', 'After'], rotation=0)  # Replace 0 and 1 with 'Before' and 'After'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fb5ea-e223-4dba-8b91-256000ddd9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jom-blockchain",
   "language": "python",
   "name": "jom-blockchain-research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
