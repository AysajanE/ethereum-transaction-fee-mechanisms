{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fcb88-e91f-466a-bd78-bd939066170d",
   "metadata": {},
   "source": [
    "# Transaction Data Collection from the Ethereum Blockchain\n",
    "\n",
    "## EIP-1559\n",
    "- Date: August 5, 2021\n",
    "- Block number: 12,965,000\n",
    "- [Ethereum JSON-RPC Specification](https://ethereum.github.io/execution-apis/api-documentation/)\n",
    "- [JSON RPC API](https://ethereum.org/en/developers/docs/apis/json-rpc/)\n",
    "- [EIP-1559 Analysis Arxiv](https://github.com/SciEcon/EIP1559)\n",
    "\n",
    "## Layer 2 Solutions Launch Dates\n",
    "Source: [L2BEAT](https://l2beat.com/scaling/tvl)\n",
    "1. Optimism is live on: January 16, 2021\n",
    "2. Arbitrum is live on: August 31, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d46b85a-a478-4e4c-b1f5-0ccc63b33ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600db6e9-0b3a-4705-975d-7b3c6fc6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Convert a date string to a Unix timestamp.\n",
    "    \n",
    "    Args:\n",
    "        date_string (str): The date string in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        int: The Unix timestamp corresponding to the date string.\n",
    "    \"\"\"\n",
    "    dt = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    return int(dt.timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7723227-ba57-44b9-b69c-fe20224672df",
   "metadata": {},
   "source": [
    "### Merge the eth tx data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9052402-ced9-411c-bd07-04c099fdd0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your files are stored\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "# Filename pattern\n",
    "filename_pattern = \"eth_transaction_data_{}.csv\"\n",
    "\n",
    "# Find all filenames in the directory\n",
    "all_files = os.listdir(data_dir)\n",
    "\n",
    "# Extract dates from filenames and convert to datetime\n",
    "dates = [datetime.strptime(re.search(r'\\d{4}-\\d{2}-\\d{2}', file).group(), \"%Y-%m-%d\") for file in all_files if re.search(r'\\d{4}-\\d{2}-\\d{2}', file)]\n",
    "\n",
    "# Find start and end dates\n",
    "start_date = min(dates)\n",
    "end_date = max(dates)\n",
    "\n",
    "print(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539db4e-cc99-41c4-afca-0acc5dd99f57",
   "metadata": {},
   "source": [
    "In this script above:\n",
    "\n",
    "1. The `os.listdir` function is used to retrieve all the filenames in the directory.\n",
    "2. The `re.search` function is used to extract the date strings from the filenames using a regular expression that matches the date format (yyyy-mm-dd).\n",
    "3. The `datetime.strptime` function is used to convert the date strings to datetime objects.\n",
    "4. The `min` and `max` functions are used to find the start and end dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ecde9-cd57-4c86-b4fb-e5c4f30eff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to indicate whether it's the first file\n",
    "first_file = True\n",
    "\n",
    "# Create or open the final CSV file in append mode\n",
    "with open('../data/merged_eth_transaction_data.csv', 'a') as singleFile:\n",
    "    for single_date in pd.date_range(start_date, end_date):\n",
    "        filename = os.path.join(data_dir, filename_pattern.format(single_date.strftime(\"%Y-%m-%d\")))\n",
    "        \n",
    "        if os.path.isfile(filename):  # if the file exists\n",
    "            df = pd.read_csv(filename, dtype={5: float})\n",
    "            # Write data to file\n",
    "            if first_file:  # If it's the first file\n",
    "                df.to_csv(singleFile, header=True)  # Write with header\n",
    "                first_file = False  # After the first file, set this flag to False\n",
    "            else:\n",
    "                df.to_csv(singleFile, header=False, mode='a')  # If not the first file, write without header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2c148-0665-4d11-8360-eaa11a737bb1",
   "metadata": {},
   "source": [
    "In this script for merging transtion data:\n",
    "1. It iterates over the date range, reads the data for each date into a DataFrame, and appends it to the final CSV file.\n",
    "2. a flag `first_file` is used to check if the current file is the first one. If it is, the script writes the DataFrame to the CSV file with headers. For subsequent files, the DataFrame is written without headers. The `mode='a'` argument to `to_csv` is used to append the data to the existing file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150d89b-a6e3-465f-88c6-ea58ee3e18b1",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Given the large size of your data, traditional Python tools like Pandas might not be able to handle it efficiently due to memory constraints. Fortunately, there are tools and libraries built specifically for handling larger-than-memory datasets, such as Dask and Vaex.\n",
    "\n",
    "1. **Dask**: Dask is a flexible library for parallel computing in Python that's built on top of existing Python APIs and data structures, like NumPy arrays and Pandas DataFrames. It can handle larger-than-memory computations by breaking them down into smaller tasks, executing these tasks in parallel and combining the results.\n",
    "\n",
    "\n",
    "2. **Vaex**: Vaex is another library for handling large datasets. Like Dask, it performs lazy evaluations and only reads in data when necessary. Its API is also similar to Pandas, which makes it easy to use if you're familiar with Pandas:\n",
    "\n",
    "In either case, you should ensure that your machine has enough storage to hold the intermediate results of the computations. Also, both Dask and Vaex make use of multiple cores, so having a multi-core machine can speed up the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000c058-698d-4b52-ae66-3798a44c77af",
   "metadata": {},
   "source": [
    "In Dask, the way to read and modify columns is similar to pandas. You can use Dask's `read_csv` function to read the CSV file and then apply the string operations to the column names using the `rename` method. Here's how you can adapt your code to use Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3578e4f4-213f-462f-8808-54a9d7cc5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Transaction Identifier', 'Block Number',\n",
      "       'Transaction Timestamp', 'Transaction Status', 'Gas Price',\n",
      "       'Transaction Fee', 'Sender's Address', 'Transaction Type',\n",
      "       'Transaction EIP-1559 Type'],\n",
      "      dtype='object')\n",
      "Unnamed: 0                     int64\n",
      "Transaction Identifier        object\n",
      "Block Number                   int64\n",
      "Transaction Timestamp          int64\n",
      "Transaction Status            object\n",
      "Gas Price                      int64\n",
      "Transaction Fee              float64\n",
      "Sender's Address              object\n",
      "Transaction Type              object\n",
      "Transaction EIP-1559 Type     object\n",
      "dtype: object\n",
      "Index(['transaction_identifier', 'block_number', 'transaction_timestamp',\n",
      "       'transaction_status', 'gas_price', 'transaction_fee', 'senders_address',\n",
      "       'transaction_type', 'transaction_eip-1559_type'],\n",
      "      dtype='object')\n",
      "transaction_identifier        object\n",
      "block_number                   int64\n",
      "transaction_timestamp          int64\n",
      "transaction_status            object\n",
      "gas_price                      int64\n",
      "transaction_fee              float64\n",
      "senders_address               object\n",
      "transaction_type              object\n",
      "transaction_eip-1559_type     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Specify the data types\n",
    "dtypes = {\n",
    "    'Transaction Identifier': str,\n",
    "    'Transaction Status': str,\n",
    "    'Sender\\'s Address': str,\n",
    "    'Transaction Type': str,\n",
    "    'Transaction EIP-1559 Type': str\n",
    "}\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = dd.read_csv('../data/merged_eth_transaction_data.csv', dtype=dtypes)\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "# If 'unnamed: 0' column exists, drop it\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Format column names to replace spaces with '_' and replace single quotes with \"\".\n",
    "df = df.rename(columns={col: col.lower().replace(' ', '_').replace(\"'\", \"\") for col in df.columns})\n",
    "\n",
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce18a66-a2ab-4d1b-b00d-046c95ec0dcc",
   "metadata": {},
   "source": [
    "Dask's `read_csv` function returns a Dask DataFrame, which is a large parallel DataFrame composed of smaller pandas DataFrames, split along the index. These pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster.\n",
    "\n",
    "One important thing to remember is that Dask uses lazy execution, which means it only executes tasks when absolutely necessary. So if you're not seeing any output or the code seems to be running very quickly, it's likely because Dask hasn't actually performed the computations yet.\n",
    "\n",
    "To actually perform the computations and get the result, you need to call the `compute` method. However, be careful when using `compute` with large datasets, because it loads the result into memory, which could cause your program to run out of memory if the result is too large. In your case, you won't need to use `compute` until you actually need to see the modified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94041f3-610b-41fe-b237-3f14427321c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_identifier</th>\n",
       "      <th>block_number</th>\n",
       "      <th>transaction_timestamp</th>\n",
       "      <th>transaction_status</th>\n",
       "      <th>gas_price</th>\n",
       "      <th>transaction_fee</th>\n",
       "      <th>senders_address</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>transaction_eip-1559_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x804bfa175a2a32b3ed330c7c642ac2210784e1646e39...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>269000000000</td>\n",
       "      <td>2.824500e+16</td>\n",
       "      <td>0xb0da6794da4E6f7244B96256AdB8973D07428a20</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2e9c29c7b00fedbf30d0e3f4c4d4039da468b46bdb27...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>246015000000</td>\n",
       "      <td>1.833599e+16</td>\n",
       "      <td>0x274F3c32C90517975e29Dfc209a23f315c1e5Fc7</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xb78c319c2e68e82be6a0fb34cd49c17e65f1304ea7dc...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>209000000000</td>\n",
       "      <td>4.389000e+15</td>\n",
       "      <td>0xf6292B422EaA44165519480Ebc232eEfB9F0a47f</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x9e26892415c6643b1728a5c6048a2521346092287276...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>209000000000</td>\n",
       "      <td>1.463000e+17</td>\n",
       "      <td>0xB630D90e3EBE206d2C2A36C060D14812E320862e</td>\n",
       "      <td>Interaction with a Contract</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xa6798928631b5e101b5dd27d123122b125e8ce5f9a6b...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>4.347000e+15</td>\n",
       "      <td>0x741c8EE1d80738dBff66cb914cF74763cc7a0f89</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x16286165e0637f5acbe2cbb2c565e3d54aa463431616...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>209000000000</td>\n",
       "      <td>4.389000e+15</td>\n",
       "      <td>0x741c8EE1d80738dBff66cb914cF74763cc7a0f89</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x9cd1cf12a60507743a89f2c3d367333bc5a89bc51adb...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>5.175000e+16</td>\n",
       "      <td>0x1A579A8FeCE2dcA0E74A12Cc350220178A353D25</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x665ff9b7c7b89fe8793eda31aecb7ff01b9723e4b7d3...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>5.175000e+16</td>\n",
       "      <td>0x662F7fEaC9401d403Ddb0972ff85270071009b9F</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x20f2c1f815341f6fa822242dc7b2aad5dbe92d1ef474...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>5.175000e+16</td>\n",
       "      <td>0x55ac3377c5B65d03866895f81B241eaF18221A8D</td>\n",
       "      <td>ERC20 Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0xa9eec9ad774dce95e9395f9787b080437bc081396af2...</td>\n",
       "      <td>11794239</td>\n",
       "      <td>1612501201</td>\n",
       "      <td>Success</td>\n",
       "      <td>207000000000</td>\n",
       "      <td>4.347000e+15</td>\n",
       "      <td>0x6fAf75112F9Ee5e7A40328F8e64041c4399a2Dc8</td>\n",
       "      <td>Simple Ether Transfer</td>\n",
       "      <td>Legacy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              transaction_identifier  block_number  \\\n",
       "0  0x804bfa175a2a32b3ed330c7c642ac2210784e1646e39...      11794239   \n",
       "1  0x2e9c29c7b00fedbf30d0e3f4c4d4039da468b46bdb27...      11794239   \n",
       "2  0xb78c319c2e68e82be6a0fb34cd49c17e65f1304ea7dc...      11794239   \n",
       "3  0x9e26892415c6643b1728a5c6048a2521346092287276...      11794239   \n",
       "4  0xa6798928631b5e101b5dd27d123122b125e8ce5f9a6b...      11794239   \n",
       "5  0x16286165e0637f5acbe2cbb2c565e3d54aa463431616...      11794239   \n",
       "6  0x9cd1cf12a60507743a89f2c3d367333bc5a89bc51adb...      11794239   \n",
       "7  0x665ff9b7c7b89fe8793eda31aecb7ff01b9723e4b7d3...      11794239   \n",
       "8  0x20f2c1f815341f6fa822242dc7b2aad5dbe92d1ef474...      11794239   \n",
       "9  0xa9eec9ad774dce95e9395f9787b080437bc081396af2...      11794239   \n",
       "\n",
       "   transaction_timestamp transaction_status     gas_price  transaction_fee  \\\n",
       "0             1612501201            Success  269000000000     2.824500e+16   \n",
       "1             1612501201            Success  246015000000     1.833599e+16   \n",
       "2             1612501201            Success  209000000000     4.389000e+15   \n",
       "3             1612501201            Success  209000000000     1.463000e+17   \n",
       "4             1612501201            Success  207000000000     4.347000e+15   \n",
       "5             1612501201            Success  209000000000     4.389000e+15   \n",
       "6             1612501201            Success  207000000000     5.175000e+16   \n",
       "7             1612501201            Success  207000000000     5.175000e+16   \n",
       "8             1612501201            Success  207000000000     5.175000e+16   \n",
       "9             1612501201            Success  207000000000     4.347000e+15   \n",
       "\n",
       "                              senders_address             transaction_type  \\\n",
       "0  0xb0da6794da4E6f7244B96256AdB8973D07428a20               ERC20 Transfer   \n",
       "1  0x274F3c32C90517975e29Dfc209a23f315c1e5Fc7               ERC20 Transfer   \n",
       "2  0xf6292B422EaA44165519480Ebc232eEfB9F0a47f        Simple Ether Transfer   \n",
       "3  0xB630D90e3EBE206d2C2A36C060D14812E320862e  Interaction with a Contract   \n",
       "4  0x741c8EE1d80738dBff66cb914cF74763cc7a0f89        Simple Ether Transfer   \n",
       "5  0x741c8EE1d80738dBff66cb914cF74763cc7a0f89        Simple Ether Transfer   \n",
       "6  0x1A579A8FeCE2dcA0E74A12Cc350220178A353D25               ERC20 Transfer   \n",
       "7  0x662F7fEaC9401d403Ddb0972ff85270071009b9F               ERC20 Transfer   \n",
       "8  0x55ac3377c5B65d03866895f81B241eaF18221A8D               ERC20 Transfer   \n",
       "9  0x6fAf75112F9Ee5e7A40328F8e64041c4399a2Dc8        Simple Ether Transfer   \n",
       "\n",
       "  transaction_eip-1559_type  \n",
       "0                    Legacy  \n",
       "1                    Legacy  \n",
       "2                    Legacy  \n",
       "3                    Legacy  \n",
       "4                    Legacy  \n",
       "5                    Legacy  \n",
       "6                    Legacy  \n",
       "7                    Legacy  \n",
       "8                    Legacy  \n",
       "9                    Legacy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe7b8d-6c54-4a91-b49f-3c08464050aa",
   "metadata": {},
   "source": [
    "### An alternative approach to merge data from multiple csv files\n",
    "The script below is more suitable for a smaller number of files or if your machine has a large amount of memory available. We will use this script to merge eth net supply data for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00426b70-d9ea-44c0-a266-0d0bb33e2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
      "/tmp/ipykernel_8766/4060746378.py:11: DtypeWarning: Columns (0,1,2,3,4,5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n"
     ]
    }
   ],
   "source": [
    "# Set the directory you want to start from\n",
    "rootDir = '../data/'  # current directory; adjust it if needed\n",
    "\n",
    "# Find all eth net supply CSV files in the directory\n",
    "all_files = glob.glob(os.path.join(rootDir, \"eth_net_supply_*.csv\"))\n",
    "\n",
    "# Use sort to ensure files are concatenated in the correct order\n",
    "all_files.sort()\n",
    "\n",
    "# Use a generator expression to lazily read and concatenate files\n",
    "merged_data = pd.concat((pd.read_csv(file) for file in all_files))\n",
    "\n",
    "dtypes = {\n",
    "    'block_number': 'int',\n",
    "    'static_reward': 'float',\n",
    "    'uncle_reward': 'float',\n",
    "    'inclusion_reward': 'float',\n",
    "    'base_fee': 'int',\n",
    "    'gas_used': 'int',\n",
    "    'burned_fee': 'int',\n",
    "    'net_supply_increase': 'float'\n",
    "}\n",
    "\n",
    "# Convert types for each column\n",
    "for column, dtype in dtypes.items():\n",
    "    # Convert the column to numeric values, making non-convertible values NaN\n",
    "    merged_data[column] = pd.to_numeric(merged_data[column], errors='coerce')\n",
    "\n",
    "    # Now, drop the rows where we got NaN values due to failed conversion\n",
    "    merged_data = merged_data[merged_data[column].notna()]\n",
    "\n",
    "    # And finally, convert the column to the target type\n",
    "    merged_data[column] = merged_data[column].astype(dtype)\n",
    "\n",
    "merged_data.to_csv('merged_eth_net_supply.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3bf509-9272-4e93-84e4-8a0516cef196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['block_number', 'static_reward', 'uncle_reward', 'inclusion_reward',\n",
      "       'base_fee', 'gas_used', 'burned_fee', 'net_supply_increase'],\n",
      "      dtype='object')\n",
      "block_number             int64\n",
      "static_reward          float64\n",
      "uncle_reward           float64\n",
      "inclusion_reward       float64\n",
      "base_fee                 int64\n",
      "gas_used                 int64\n",
      "burned_fee               int64\n",
      "net_supply_increase    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.columns)\n",
    "print(merged_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18f042f4-c123-452c-ab88-d3ef539b03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_senders(input_file, output_file, sender_column):\n",
    "    \"\"\"\n",
    "    Function to extract unique sender addresses from a large CSV file of transaction data using Dask.\n",
    "\n",
    "    Parameters:\n",
    "    input_file (str): Path to the input CSV file representing transaction data.\n",
    "    output_file (str): Path to the output CSV file where unique sender addresses will be saved.\n",
    "    sender_column (str): The name of the column in the input file that contains the sender's addresses.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in data using Dask's read_csv function\n",
    "    # Dask is a parallel computing library that allows us to work with large datasets\n",
    "    # The read_csv function works similarly to pandas' read_csv, but it performs the operations lazily\n",
    "    df = dd.read_csv(input_file)\n",
    "\n",
    "    # Get unique sender's addresses\n",
    "    # drop_duplicates returns the unique values in the sender_column\n",
    "    unique_senders = df[sender_column].drop_duplicates()\n",
    "\n",
    "    # Compute the result and save to a new CSV file\n",
    "    # compute() performs the actual computation and returns a pandas DataFrame\n",
    "    # to_csv writes the DataFrame to a CSV file\n",
    "    unique_senders.compute().to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca46eca-7ea9-4431-8950-24c15aee8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "extract_unique_senders('../data/merged_eth_transaction_data.csv', 'unique_senders.csv', 'Sender\\'s Address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aead7fb9-b795-4196-a708-81f87575f217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/aeziz-local/Research/@2023/blockchain-JOM-special-issue-Sep30/ethereum-transaction-fee-mechanisms/data/unique_senders.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory where your files are stored\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = dd.read_csv(f\"{data_dir}unique_senders.csv\")\n",
    "# Format column names to replace spaces with '_' and replace single quotes with \"\".\n",
    "df = df.rename(columns={col: col.lower().replace(' ', '_').replace(\"'\", \"\") for col in df.columns})\n",
    "# Overwrite the original CSV file\n",
    "df.to_csv(f\"{data_dir}unique_senders.csv\", index=False, single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07633bb-3466-41c4-960f-907f02f511d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jom-blockchain",
   "language": "python",
   "name": "jom-blockchain-research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
